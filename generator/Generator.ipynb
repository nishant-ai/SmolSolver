{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T03:17:47.513354Z",
     "iopub.status.busy": "2025-11-04T03:17:47.513202Z",
     "iopub.status.idle": "2025-11-04T03:19:44.953086Z",
     "shell.execute_reply": "2025-11-04T03:19:44.952213Z",
     "shell.execute_reply.started": "2025-11-04T03:17:47.513340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mTue Nov  4 03:19:18 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   43C    P8             11W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   41C    P8             11W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 03:19:33.240577: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762226373.408248      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762226373.454920      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.57.1\n",
      "datasets: 4.1.1\n",
      "peft: 0.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets peft bitsandbytes accelerate wandb trl scipy\n",
    "\n",
    "# Verify GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Fix seeds for reproducibility\n",
    "import torch, random, numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Log versions\n",
    "import transformers, datasets, peft\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"datasets: {datasets.__version__}\")\n",
    "print(f\"peft: {peft.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T03:19:44.957833Z",
     "iopub.status.busy": "2025-11-04T03:19:44.957275Z",
     "iopub.status.idle": "2025-11-04T03:19:48.395587Z",
     "shell.execute_reply": "2025-11-04T03:19:48.394779Z",
     "shell.execute_reply.started": "2025-11-04T03:19:44.957812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37625577967e4c3fa083173587a217a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2ad3835bed4fa3a1462a62bdbcf348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb94f1a81a1c487aab584b0ed8dbc967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499ecec2228644e5badbe6c29150451d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7caa0874985413eb9a7f16dab44bf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6725, Val: 748, Test: 1319\n",
      "\n",
      "Sample from training set:\n",
      "{'question': 'A kilogram of pork costs $6 while a kilogram of chicken costs $2 less. How much will a 3-kilogram of chicken and a kilogram of pork cost?', 'answer': 'A kilogram of chicken costs $6 - $2 = $<<6-2=4>>4.\\nThree kilograms of chicken cost $4 x 3 = $<<4*3=12>>12.\\nSo, a 3-kilogram of chicken and a kilogram of pork cost $12 + $6 = $18.\\n#### 18'}\n"
     ]
    }
   ],
   "source": [
    "  from datasets import load_dataset\n",
    "  import random\n",
    "  import numpy as np\n",
    "\n",
    "  # Set seed for reproducibility\n",
    "  random.seed(42)\n",
    "  np.random.seed(42)\n",
    "\n",
    "  # Load GSM8K\n",
    "  dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "  # Keep test set UNTOUCHED for final evaluation\n",
    "  test_set = dataset[\"test\"]  # 1,319 examples - DO NOT USE YET\n",
    "\n",
    "  # Split train (7,473) into train/val (90/10)\n",
    "  train_val = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "  train_set = train_val[\"train\"]  # ~6,726 examples\n",
    "  val_set = train_val[\"test\"]     # ~747 examples\n",
    "\n",
    "  print(f\"Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}\")\n",
    "  print(\"\\nSample from training set:\")\n",
    "  print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T03:19:48.397085Z",
     "iopub.status.busy": "2025-11-04T03:19:48.396409Z",
     "iopub.status.idle": "2025-11-04T03:19:49.125943Z",
     "shell.execute_reply": "2025-11-04T03:19:49.125045Z",
     "shell.execute_reply.started": "2025-11-04T03:19:48.397060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9cb8fdad4e499987c0c35678b02af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2bee640ed84d60bf81cab5835f7f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b36bdac3d1a41178e067c642aa23863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c89f3fa39d14cbf98cd1f5bd10783ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def format_gsm8k_example(example):\n",
    "  \"\"\"\n",
    "  Format: \n",
    "  - System prompt enforcing â‰¤12 steps\n",
    "  - Numbered atomic steps\n",
    "  - FINAL_ANSWER: <number>\n",
    "  \"\"\"\n",
    "  question = example[\"question\"]\n",
    "  solution = example[\"answer\"]  # Original solution\n",
    "\n",
    "  # Extract ground truth answer (after ####)\n",
    "  ground_truth = solution.split(\"####\")[-1].strip()\n",
    "\n",
    "  # Parse solution into atomic steps (simplified - you'll refine this)\n",
    "  steps = solution.split(\"\\n\")\n",
    "  steps = [s.strip() for s in steps if s.strip() and \"####\" not in s]\n",
    "\n",
    "  # Format as numbered steps (limit to 12)\n",
    "  formatted_steps = \"\\n\".join([f\"{i+1}) {step}\" for i, step in enumerate(steps[:12])])\n",
    "\n",
    "  # Create training example\n",
    "  prompt = f\"\"\"Solve the following math problem step-by-step. Use at most 12 numbered steps. End with FINAL_ANSWER: <number>.\n",
    "\n",
    "Problem:\n",
    "{question}\n",
    "\n",
    "Solution:\"\"\"\n",
    "\n",
    "  response = f\"\"\"{formatted_steps}\n",
    "FINAL_ANSWER: {ground_truth}\"\"\"\n",
    "\n",
    "  return {\n",
    "      \"prompt\": prompt,\n",
    "      \"response\": response,\n",
    "      \"question\": question,\n",
    "      \"answer\": ground_truth\n",
    "  }\n",
    "\n",
    "# Apply formatting\n",
    "train_formatted = train_set.map(format_gsm8k_example)\n",
    "val_formatted = val_set.map(format_gsm8k_example)\n",
    "\n",
    "\n",
    "\n",
    "def validate_example(example):\n",
    "  \"\"\"Ensure quality: â‰¤12 steps, has FINAL_ANSWER, numeric answer\"\"\"\n",
    "  response = example[\"response\"]\n",
    "\n",
    "  # Check for FINAL_ANSWER\n",
    "  if \"FINAL_ANSWER:\" not in response:\n",
    "      return False\n",
    "\n",
    "  # Count steps\n",
    "  step_count = len([line for line in response.split(\"\\n\") if line.strip() and line[0].isdigit()])\n",
    "  if step_count > 12:\n",
    "      return False\n",
    "\n",
    "  # Check numeric answer exists\n",
    "  answer = response.split(\"FINAL_ANSWER:\")[-1].strip()\n",
    "  if not any(char.isdigit() for char in answer):\n",
    "      return False\n",
    "\n",
    "  return True\n",
    "\n",
    "# Filter invalid examples\n",
    "train_formatted = train_formatted.filter(validate_example)\n",
    "val_formatted = val_formatted.filter(validate_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T03:19:49.126943Z",
     "iopub.status.busy": "2025-11-04T03:19:49.126736Z",
     "iopub.status.idle": "2025-11-04T03:20:32.290353Z",
     "shell.execute_reply": "2025-11-04T03:20:32.289652Z",
     "shell.execute_reply.started": "2025-11-04T03:19:49.126926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58044ab7b5404438af7c7dd347ddbe02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dee96efb3504dbc81ae239d10dae350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357928ed98c44e2a8aa35407effbf165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb81389b5254c509196479117c50dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becd8cdef951432897e094cfb676c0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24af713f02f43d8b455fcc4a1c3309e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3153c71d835b4da8b58c97fa8f78fbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81c13a49254424d8620c9f52e79a679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad80aabe761489d8777ff38146a76aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2086339c9019448e9a0916625a48ab81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84cb666aa264ce38f9303c2b4ad83d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a2e696336d404f9af880fb335561d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8178c2b4a24a619100f0f67ca76fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,864,320 || all params: 2,787,548,160 || trainable%: 0.2821\n",
      "\n",
      "âœ… Model loaded with optimized LoRA (r=16, alpha=16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "# Optimized QLoRA for T4 (fp16)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_use_double_quant=True,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "  bnb_4bit_compute_dtype=torch.float16,  # fp16 for T4\n",
    ")\n",
    "\n",
    "# Load Phi-2\n",
    "model_name = \"microsoft/phi-2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  quantization_config=bnb_config,\n",
    "  device_map=\"auto\",\n",
    "  trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Optimized LoRA: r=16, alpha=16\n",
    "lora_config = LoraConfig(\n",
    "  r=16,\n",
    "  lora_alpha=16,\n",
    "  lora_dropout=0.1,\n",
    "  bias=\"none\",\n",
    "  target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "  task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nâœ… Model loaded with optimized LoRA (r=16, alpha=16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T03:20:32.292052Z",
     "iopub.status.busy": "2025-11-04T03:20:32.291847Z",
     "iopub.status.idle": "2025-11-04T05:13:05.105484Z",
     "shell.execute_reply": "2025-11-04T05:13:05.104491Z",
     "shell.execute_reply.started": "2025-11-04T03:20:32.292037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6d708fb639446c984db749a159a3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing training set:   0%|          | 0/6725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d691fb5bbfd64fce908d9bf7765989b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation set:   0%|          | 0/748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenized 6725 training examples\n",
      "âœ… Tokenized 748 validation examples\n",
      "\n",
      "============================================================\n",
      "TRAINING CONFIGURATION\n",
      "============================================================\n",
      "Training examples: 6725\n",
      "Validation examples: 748\n",
      "Effective batch size: 32\n",
      "Total epochs: 2\n",
      "Estimated steps per epoch: ~210\n",
      "Total training steps: ~420\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='422' max='422' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [422/422 1:52:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.540900</td>\n",
       "      <td>0.493366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.525200</td>\n",
       "      <td>0.486181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training complete!\n",
      "âœ… Model saved to ./generator_final\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "\n",
    "# Make sure the tokenizer has a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MAX_LEN = 1024\n",
    "\n",
    "def format_and_tokenize(batch):\n",
    "  \"\"\"\n",
    "  Tokenize and mask prompt tokens so loss is only computed on the response.\n",
    "  This is the standard approach for instruction fine-tuning.\n",
    "  \"\"\"\n",
    "  inputs = []\n",
    "  attn = []\n",
    "  labels = []\n",
    "\n",
    "  for p, r in zip(batch[\"prompt\"], batch[\"response\"]):\n",
    "      # Tokenize full text (prompt + response)\n",
    "      full = p + \" \" + r\n",
    "      tok_full = tokenizer(full, truncation=True, max_length=MAX_LEN, padding=False)\n",
    "\n",
    "      # Tokenize just the prompt to find where it ends\n",
    "      tok_prompt = tokenizer(p, truncation=True, max_length=MAX_LEN, padding=False)\n",
    "\n",
    "      # Create labels: mask prompt tokens with -100 (ignored in loss)\n",
    "      lab = tok_full[\"input_ids\"][:]\n",
    "      prompt_len = len(tok_prompt[\"input_ids\"])\n",
    "      lab[:prompt_len] = [-100] * prompt_len\n",
    "\n",
    "      inputs.append(tok_full[\"input_ids\"])\n",
    "      attn.append(tok_full[\"attention_mask\"])\n",
    "      labels.append(lab)\n",
    "\n",
    "  return {\"input_ids\": inputs, \"attention_mask\": attn, \"labels\": labels}\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_formatted.map(\n",
    "  format_and_tokenize,\n",
    "  batched=True,\n",
    "  remove_columns=train_formatted.column_names,\n",
    "  desc=\"Tokenizing training set\"\n",
    ")\n",
    "tokenized_val = val_formatted.map(\n",
    "  format_and_tokenize,\n",
    "  batched=True,\n",
    "  remove_columns=val_formatted.column_names,\n",
    "  desc=\"Tokenizing validation set\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Tokenized {len(tokenized_train)} training examples\")\n",
    "print(f\"âœ… Tokenized {len(tokenized_val)} validation examples\")\n",
    "\n",
    "# âœ… FIXED: Use DataCollatorForSeq2Seq instead of default_data_collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "  tokenizer=tokenizer,\n",
    "  model=model,\n",
    "  padding=True,\n",
    "  pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./generator_checkpoints\",\n",
    "  num_train_epochs=2,\n",
    "  per_device_train_batch_size=1,\n",
    "  gradient_accumulation_steps=32,\n",
    "  learning_rate=1e-4,\n",
    "  lr_scheduler_type=\"cosine\",\n",
    "  warmup_ratio=0.03,\n",
    "  fp16=True,\n",
    "  gradient_checkpointing=True,\n",
    "  max_grad_norm=1.0,\n",
    "\n",
    "  eval_strategy=\"steps\",\n",
    "  eval_steps=200,\n",
    "  logging_steps=50,\n",
    "  save_strategy=\"steps\",\n",
    "  save_steps=200,\n",
    "  save_total_limit=2,\n",
    "\n",
    "  report_to=\"none\",\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"loss\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_train,\n",
    "  eval_dataset=tokenized_val,\n",
    "  data_collator=data_collator,  # â† Using the fixed collator\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training examples: {len(tokenized_train)}\")\n",
    "print(f\"Validation examples: {len(tokenized_val)}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Total epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Estimated steps per epoch: ~{len(tokenized_train) // 32}\")\n",
    "print(f\"Total training steps: ~{(len(tokenized_train) // 32) * 2}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# âœ… Resume from checkpoint to save time!\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(\"./generator_final\")\n",
    "tokenizer.save_pretrained(\"./generator_final\")\n",
    "print(\"âœ… Model saved to ./generator_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T19:30:03.696431Z",
     "iopub.status.busy": "2025-11-03T19:30:03.695839Z",
     "iopub.status.idle": "2025-11-03T19:30:03.702160Z",
     "shell.execute_reply": "2025-11-03T19:30:03.701310Z",
     "shell.execute_reply.started": "2025-11-03T19:30:03.696405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns: ['question', 'answer', 'prompt', 'response']\n",
      "\n",
      "First example keys: dict_keys(['question', 'answer', 'prompt', 'response'])\n",
      "\n",
      "Sample data:\n",
      "{'question': 'A kilogram of pork costs $6 while a kilogram of chicken costs $2 less. How much will a 3-kilogram of chicken and a kilogram of pork cost?', 'answer': '18', 'prompt': 'Solve the following math problem step-by-step. Use at most 12 numbered steps. End with FINAL_ANSWER: <number>.\\n\\nProblem:\\nA kilogram of pork costs $6 while a kilogram of chicken costs $2 less. How much will a 3-kilogram of chicken and a kilogram of pork cost?\\n\\nSolution:', 'response': '1) A kilogram of chicken costs $6 - $2 = $<<6-2=4>>4.\\n2) Three kilograms of chicken cost $4 x 3 = $<<4*3=12>>12.\\n3) So, a 3-kilogram of chicken and a kilogram of pork cost $12 + $6 = $18.\\nFINAL_ANSWER: 18'}\n"
     ]
    }
   ],
   "source": [
    "  print(\"Train columns:\", train_formatted.column_names)\n",
    "  print(\"\\nFirst example keys:\", train_formatted[0].keys())\n",
    "  print(\"\\nSample data:\")\n",
    "  print(train_formatted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T05:46:35.810342Z",
     "iopub.status.busy": "2025-11-04T05:46:35.810026Z",
     "iopub.status.idle": "2025-11-04T07:11:39.516620Z",
     "shell.execute_reply": "2025-11-04T07:11:39.515710Z",
     "shell.execute_reply.started": "2025-11-04T05:46:35.810323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be36c684e724fbfaa910ff88d67534d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded\n",
      "\n",
      "============================================================\n",
      "EVALUATION: Pass@1 on Validation Set (748 examples)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 1/748 [00:05<1:10:12,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Example 1:\n",
      "Q: Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them...\n",
      "Ground Truth: 16.0\n",
      "Predicted: 16.0\n",
      "Correct: âœ…\n",
      "Solution:\n",
      "1) Mimi picked up 2 x 12 = <<2*12=24>>24 seashells.\n",
      "2) Kyle found twice as many shells as Mimi, so he found 2 x 24 = <<2*24=48>>48 shells.\n",
      "3) Leigh grabbed one-third of the shells that Kyle found, so she had 48 / 3 = <<48/3=16>>16 shells.\n",
      "FINAL_ANSWER: 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 2/748 [00:19<2:12:59, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Example 2:\n",
      "Q: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less p...\n",
      "Ground Truth: 19.0\n",
      "Predicted: 12.0\n",
      "Correct: âŒ\n",
      "Solution:\n",
      "1) He has 2 dogs and 6 more snakes than cats, so he has 2 + 6 = <<2+6=8>>8 snakes.\n",
      "2) He has one less parrot than cats, so he has 2 - 1 = <<2-1=1>>1 parrot.\n",
      "3) He has 6 pets with four legs, so he has 6 * 4 = <<6*4=24>>24 legs.\n",
      "4) He has 2 dogs and 1 parrot, so he has 2 + 1 = <<2+1=3>>3 pets with two legs.\n",
      "5) He has 8 snakes and 1 parrot, so he has 8 + 1 = <<8+1=9>>9 pets with four legs.\n",
      "6) He has ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 3/748 [00:29<2:09:12, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Example 3:\n",
      "Q: Olaf collects colorful toy cars. At first, his collection consisted of 150 cars. His family, knowing...\n",
      "Ground Truth: 196.0\n",
      "Predicted: 536.0\n",
      "Correct: âŒ\n",
      "Solution:\n",
      "1) Olaf's uncle gave him 150/2 = <<150/2=75>>75 toy cars.\n",
      "2) Olaf's aunt gave him 75+1 = <<75+1=76>>76 toy cars.\n",
      "3) Olaf's dad gave him 10+5 = <<10+5=15>>15 toy cars.\n",
      "4) Olaf's mom gave him 15+5 = <<15+5=20>>20 toy cars.\n",
      "5) Olaf's grandpa gave him 75*2 = <<75*2=150>>150 toy cars.\n",
      "6) Olaf has 150+75+76+15+20+150 = <<150+75+76+15+20+150=536>>536 toy cars.\n",
      "FINAL_ANSWER: 536...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 4/748 [00:37<1:53:08,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Example 4:\n",
      "Q: Emma's bank account has $100 in it. Each day of the week, she spends $8. At the end of the week, she...\n",
      "Ground Truth: 4.0\n",
      "Predicted: 24.0\n",
      "Correct: âŒ\n",
      "Solution:\n",
      "1) Emma spends $8 x 7 = $<<8*7=56>>56 in a week.\n",
      "2) She has $100 - $56 = $<<100-56=44>>44 left in the account.\n",
      "3) She can get $44 / $5 = <<44/5=8.8>>8.8 $5 bills.\n",
      "4) She can get 8 $5 bills and $44 - 8*$5 = $<<44-8*5=24>>24 remain in the account.\n",
      "FINAL_ANSWER: 24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 5/748 [00:42<1:37:54,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Example 5:\n",
      "Q: Ezekiel hikes as a hobby. This past summer, he did a challenging three-day hike across 50 kilometers...\n",
      "Ground Truth: 15.0\n",
      "Predicted: 15.0\n",
      "Correct: âœ…\n",
      "Solution:\n",
      "1) The second day was half the full hike distance, so he hiked 50/2 = <<50/2=25>>25 kilometers.\n",
      "2) He hiked 10 + 25 = <<10+25=35>>35 kilometers on the first two days.\n",
      "3) He had to hike 50 - 35 = <<50-35=15>>15 kilometers on the third day to finish the hike.\n",
      "FINAL_ANSWER: 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 748/748 [1:25:00<00:00,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULTS - MILESTONE 1\n",
      "============================================================\n",
      "Pass@1 Accuracy: 64.06%\n",
      "Correct: 476/743\n",
      "Valid Format Rate: 99.33%\n",
      "Invalid Format: 5\n",
      "============================================================\n",
      "\n",
      "âœ… Results saved to eval_results.json\n",
      "âœ… MILESTONE 1 COMPLETE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "  from peft import PeftModel\n",
    "  from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "  import torch\n",
    "  import re\n",
    "  from tqdm import tqdm\n",
    "\n",
    "  # Load trained model\n",
    "  print(\"Loading trained model...\")\n",
    "  base_model_eval = AutoModelForCausalLM.from_pretrained(\n",
    "      \"microsoft/phi-2\",\n",
    "      device_map=\"auto\",\n",
    "      trust_remote_code=True,\n",
    "      dtype=torch.float16,\n",
    "  )\n",
    "  model_eval = PeftModel.from_pretrained(base_model_eval, \"./generator_final\")\n",
    "  model_eval.eval()\n",
    "  tokenizer_eval = AutoTokenizer.from_pretrained(\"./generator_final\", trust_remote_code=True)\n",
    "  print(\"âœ… Model loaded\\n\")\n",
    "\n",
    "  def generate_solution(model, tokenizer, question, max_new_tokens=512):\n",
    "      \"\"\"Generate with deterministic decoding\"\"\"\n",
    "      prompt = f\"\"\"Solve the following math problem step-by-step. Use at most 12 numbered steps. End with FINAL_ANSWER: <number>.\n",
    "\n",
    "  Problem:\n",
    "  {question}\n",
    "\n",
    "  Solution:\"\"\"\n",
    "\n",
    "      inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "      with torch.no_grad():\n",
    "          outputs = model.generate(\n",
    "              **inputs,\n",
    "              max_new_tokens=max_new_tokens,\n",
    "              temperature=0.0,\n",
    "              do_sample=False,\n",
    "              pad_token_id=tokenizer.eos_token_id,\n",
    "          )\n",
    "\n",
    "      generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "      solution = generated.split(\"Solution:\")[-1].strip()\n",
    "\n",
    "      return solution\n",
    "\n",
    "  def extract_answer(solution_text):\n",
    "      \"\"\"Extract numeric answer after FINAL_ANSWER:\"\"\"\n",
    "      match = re.search(r'FINAL_ANSWER:\\s*([^\\n]+)', solution_text)\n",
    "      if not match:\n",
    "          return None\n",
    "\n",
    "      answer_str = match.group(1).strip()\n",
    "      answer_str = re.sub(r'[$,]', '', answer_str)\n",
    "\n",
    "      num_match = re.search(r'-?\\d+\\.?\\d*', answer_str)\n",
    "      if num_match:\n",
    "          return float(num_match.group())\n",
    "\n",
    "      return None\n",
    "\n",
    "  def evaluate_pass_at_1(model, tokenizer, dataset, show_examples=5):\n",
    "      \"\"\"Evaluate Pass@1 accuracy\"\"\"\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      invalid_format = 0\n",
    "\n",
    "      for i, example in enumerate(tqdm(dataset, desc=\"Evaluating\")):\n",
    "          question = example[\"question\"]\n",
    "\n",
    "          ground_truth_str = str(example[\"answer\"])\n",
    "          ground_truth_str = re.sub(r'[$,]', '', ground_truth_str)\n",
    "\n",
    "          try:\n",
    "              ground_truth = float(ground_truth_str)\n",
    "          except:\n",
    "              continue\n",
    "\n",
    "          solution = generate_solution(model, tokenizer, question)\n",
    "          predicted = extract_answer(solution)\n",
    "\n",
    "          if predicted is None:\n",
    "              invalid_format += 1\n",
    "              continue\n",
    "\n",
    "          is_correct = abs(predicted - ground_truth) < 1e-4\n",
    "          if is_correct:\n",
    "              correct += 1\n",
    "\n",
    "          total += 1\n",
    "\n",
    "          if i < show_examples:\n",
    "              print(f\"\\n{'='*60}\")\n",
    "              print(f\"Example {i+1}:\")\n",
    "              print(f\"Q: {question[:100]}...\")\n",
    "              print(f\"Ground Truth: {ground_truth}\")\n",
    "              print(f\"Predicted: {predicted}\")\n",
    "              print(f\"Correct: {'âœ…' if is_correct else 'âŒ'}\")\n",
    "              print(f\"Solution:\\n{solution[:400]}...\")\n",
    "\n",
    "      accuracy = correct / total if total > 0 else 0\n",
    "      format_rate = (total / len(dataset)) * 100\n",
    "\n",
    "      return {\n",
    "          \"accuracy\": accuracy,\n",
    "          \"correct\": correct,\n",
    "          \"total\": total,\n",
    "          \"format_rate\": format_rate,\n",
    "          \"invalid_format\": invalid_format\n",
    "      }\n",
    "\n",
    "  print(\"=\"*60)\n",
    "  print(\"EVALUATION: Pass@1 on Validation Set (748 examples)\")\n",
    "  print(\"=\"*60)\n",
    "\n",
    "  results = evaluate_pass_at_1(model_eval, tokenizer_eval, val_formatted)\n",
    "\n",
    "  print(f\"\\n{'='*60}\")\n",
    "  print(\"FINAL RESULTS - MILESTONE 1\")\n",
    "  print(\"=\"*60)\n",
    "  print(f\"Pass@1 Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "  print(f\"Correct: {results['correct']}/{results['total']}\")\n",
    "  print(f\"Valid Format Rate: {results['format_rate']:.2f}%\")\n",
    "  print(f\"Invalid Format: {results['invalid_format']}\")\n",
    "  print(\"=\"*60)\n",
    "\n",
    "  import json\n",
    "  with open(\"eval_results.json\", \"w\") as f:\n",
    "      json.dump(results, f, indent=2)\n",
    "  print(\"\\nâœ… Results saved to eval_results.json\")\n",
    "  print(\"âœ… MILESTONE 1 COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T05:30:47.189620Z",
     "iopub.status.busy": "2025-11-04T05:30:47.188749Z",
     "iopub.status.idle": "2025-11-04T05:30:47.194491Z",
     "shell.execute_reply": "2025-11-04T05:30:47.193850Z",
     "shell.execute_reply.started": "2025-11-04T05:30:47.189593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in val_formatted: ['question', 'answer', 'prompt', 'response']\n",
      "\n",
      "First example:\n",
      "{'question': 'Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?', 'answer': '16', 'prompt': 'Solve the following math problem step-by-step. Use at most 12 numbered steps. End with FINAL_ANSWER: <number>.\\n\\nProblem:\\nMimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?\\n\\nSolution:', 'response': '1) Mimi has 2 x 12 = <<2*12=24>>24 sea shells.\\n2) Kyle has 24 x 2 = <<24*2=48>>48 sea shells.\\n3) Leigh has 48 / 3 = <<48/3=16>>16 sea shells.\\nFINAL_ANSWER: 16'}\n"
     ]
    }
   ],
   "source": [
    "  print(\"Columns in val_formatted:\", val_formatted.column_names)\n",
    "  print(\"\\nFirst example:\")\n",
    "  print(val_formatted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T07:20:31.712764Z",
     "iopub.status.busy": "2025-11-04T07:20:31.712170Z",
     "iopub.status.idle": "2025-11-04T07:20:33.501723Z",
     "shell.execute_reply": "2025-11-04T07:20:33.500924Z",
     "shell.execute_reply.started": "2025-11-04T07:20:31.712740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created milestone1_deliverables.zip\n",
      "ğŸ“¦ Download this file (right-click â†’ Download)\n",
      "ğŸ“Š Size: 28.9 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Create a directory with essential files\n",
    "os.makedirs(\"milestone1_deliverables\", exist_ok=True)\n",
    "\n",
    "# Copy generator_final\n",
    "shutil.copytree(\"./generator_final\", \"./milestone1_deliverables/generator_final\")\n",
    "\n",
    "# Copy eval results\n",
    "shutil.copy(\"eval_results.json\", \"./milestone1_deliverables/\")\n",
    "\n",
    "# Create a summary file\n",
    "summary = f\"\"\"\n",
    "MILESTONE 1 - Generator SLM Baseline\n",
    "=====================================\n",
    "\n",
    "Model: Phi-2 (2.7B) with QLoRA\n",
    "Training: 2 epochs, ~2 hours\n",
    "Pass@1 Accuracy: 64.06%\n",
    "Correct: 476/743\n",
    "Format Rate: 99.33%\n",
    "\n",
    "Configuration:\n",
    "- LoRA r=16, alpha=16\n",
    "- Learning rate: 1e-4\n",
    "- Batch size: 32 (effective)\n",
    "- Max length: 1024\n",
    "- Precision: FP16\n",
    "\n",
    "Date: Nov 4, 2025\n",
    "Team: SmolSolver\n",
    "\"\"\"\n",
    "\n",
    "with open(\"./milestone1_deliverables/SUMMARY.txt\", \"w\") as f:\n",
    "  f.write(summary)\n",
    "\n",
    "# Create zip\n",
    "shutil.make_archive(\"milestone1_deliverables\", \"zip\", \"./milestone1_deliverables\")\n",
    "\n",
    "print(\"âœ… Created milestone1_deliverables.zip\")\n",
    "print(\"ğŸ“¦ Download this file (right-click â†’ Download)\")\n",
    "print(f\"ğŸ“Š Size: {os.path.getsize('milestone1_deliverables.zip') / 1024 / 1024:.1f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
